{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LeNet5_Pytorch.ipynb","version":"0.3.2","provenance":[{"file_id":"1XcoYe0Hrhei7JF_ui77z1uZaRh7ZI3io","timestamp":1551284165032},{"file_id":"1G6lHCNMHgmyA4ftoppMZXokc9dA4bvKS","timestamp":1551283788140}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"RHovbbXa12c9","colab_type":"text"},"cell_type":"markdown","source":["# LeNet5 using PyTorch\n","\n","The data set is the MNIST data set, and the CNN model is LeNet.\n","\n","**Reference**:\n","\n","[1] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. \"Gradient-based learning applied to document recognition.\" Proceedings of the IEEE, 86(11):2278-2324, November 1998.\n","\n","[2] https://github.com/salmanazarr/LeNet-5\n","\n"]},{"metadata":{"id":"glnh8VaNLeJc","colab_type":"text"},"cell_type":"markdown","source":["# Data Set\n","\n","Explore the data set"]},{"metadata":{"id":"KxnqlJDa11yr","colab_type":"code","outputId":"31a6f153-f828-47f2-cbff-90016d43f3c7","executionInfo":{"status":"ok","timestamp":1551279246822,"user_tz":0,"elapsed":8073,"user":{"displayName":"Chris Irving","photoUrl":"https://lh5.googleusercontent.com/-Aq0UKUEzxlw/AAAAAAAAAAI/AAAAAAAAAB4/zjKS1KNld7M/s64/photo.jpg","userId":"00600840190530285990"}},"colab":{"base_uri":"https://localhost:8080/","height":370}},"cell_type":"code","source":["# MNIST data set \n","%matplotlib inline\n","\n","# Plot ad hoc mnist instances\n","from torchvision.datasets import MNIST\n","import matplotlib.pyplot as plt\n","\n","# load (download if needed) the MNIST dataset\n","mnist_train = MNIST(\".\", train=True, download=True)\n","\n","# plot 4 images as gray scale\n","plt.subplot(221)\n","plt.imshow(mnist_train.train_data[0], cmap=plt.get_cmap('gray'))\n","plt.subplot(222)\n","plt.imshow(mnist_train.train_data[1], cmap=plt.get_cmap('gray'))\n","plt.subplot(223)\n","plt.imshow(mnist_train.train_data[2], cmap=plt.get_cmap('gray'))\n","plt.subplot(224)\n","plt.imshow(mnist_train.train_data[3], cmap=plt.get_cmap('gray'))\n","\n","# show the plot\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Processing...\n","Done!\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAToAAAD7CAYAAAD6gVj5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 432x288 with 4 Axes>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"1x76z5tv1tG1","colab_type":"code","outputId":"b812a7a0-6caf-4341-9d45-c9c9e54100d0","executionInfo":{"status":"ok","timestamp":1551183900460,"user_tz":0,"elapsed":566,"user":{"displayName":"Chris Irving","photoUrl":"https://lh5.googleusercontent.com/-Aq0UKUEzxlw/AAAAAAAAAAI/AAAAAAAAAB4/zjKS1KNld7M/s64/photo.jpg","userId":"00600840190530285990"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["# to show the picture.be careful to add the .train_data\n","mnist_train.train_data.size()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([60000, 28, 28])"]},"metadata":{"tags":[]},"execution_count":2}]},{"metadata":{"id":"HXBZpUx1JlGf","colab_type":"text"},"cell_type":"markdown","source":["## Network\n","\n","Implement the LeNet5 network\n","\n","In this block, I defined the nerual network."]},{"metadata":{"id":"QoUc5gXz4y87","colab_type":"code","colab":{}},"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from collections import OrderedDict\n","\n","class LeNet5(nn.Module):\n","    '''\n","    LeNet5:\n","\n","    Input: 1*32*32\n","\n","    C1- 6@28*28(5*5 kernel)\n","\n","    tanh/ Relu\n","\n","    S2- 6@14*14(2*2 kernel, stride=2) pooling layer\n","\n","    C3- 16@10*10(5*5 kernel, stride=1) \n","\n","    tanh/Relu\n","\n","    S4- 16@5*5 (2*2 kernel, stride=2) pooling layer\n","\n","    C5- 120@1*1 (5*5 kernel, no stride) --make it into fully connected\n","\n","    tanh/Relu\n","\n","    F6- 84 tanh/Relu \n","\n","    F7- 10 Softmax output \n","\n","    '''  \n","    def __init__(self):\n","        super(LeNet5, self).__init__()\n","\n","        self.convnet = nn.Sequential(OrderedDict([\n","            ('c1', nn.Conv2d(1, 6, kernel_size=(5, 5))),\n","            ('relu1', nn.ReLU()),\n","            ('s2', nn.MaxPool2d(kernel_size=(2, 2), stride=2)),\n","            ('c3', nn.Conv2d(6, 16, kernel_size=(5, 5))),\n","            ('relu3', nn.ReLU()),\n","            ('s4', nn.MaxPool2d(kernel_size=(2, 2), stride=2)),\n","            ('c5', nn.Conv2d(16, 120, kernel_size=(5, 5))),\n","            ('relu5', nn.ReLU())\n","        ]))\n","\n","        self.fc = nn.Sequential(OrderedDict([\n","            ('f6', nn.Linear(120, 84)),\n","            ('relu6', nn.ReLU()),\n","            ('f7', nn.Linear(84, 10)),\n","            ('sig7', nn.LogSoftmax(dim=-1))\n","        ]))\n","\n","    def forward(self, img):\n","        output = self.convnet(img)\n","        output = output.view(img.size(0), -1)\n","        output = self.fc(output)\n","        return output"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zVuI3820Llh7","colab_type":"text"},"cell_type":"markdown","source":["# Processing and Run the Network\n","\n"]},{"metadata":{"id":"tTuFa8m4c4kO","colab_type":"code","colab":{}},"cell_type":"code","source":["import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","# import visdom\n","import torch.optim as optim"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KemfSYoA11Rq","colab_type":"code","colab":{}},"cell_type":"code","source":["data_train = MNIST('.',\n","                   train=True,\n","                   download=True,\n","                   transform=transforms.Compose([\n","                       transforms.Resize((32, 32)),\n","                       transforms.ToTensor()]))\n","data_test = MNIST('.',\n","                  train=False,\n","                  download=True,\n","                  transform=transforms.Compose([\n","                      transforms.Resize((32, 32)),\n","                      transforms.ToTensor()]))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PLdbInm3dBuH","colab_type":"code","colab":{}},"cell_type":"code","source":["data_train_loader = DataLoader(data_train, batch_size=256, shuffle=True, num_workers=8)\n","data_test_loader = DataLoader(data_test, batch_size=1024, num_workers=8) \n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"V0EHSfNfdelJ","colab_type":"code","outputId":"9d4912ed-cce1-436a-a211-47ddcc301a2a","executionInfo":{"status":"ok","timestamp":1551281147667,"user_tz":0,"elapsed":450009,"user":{"displayName":"Chris Irving","photoUrl":"https://lh5.googleusercontent.com/-Aq0UKUEzxlw/AAAAAAAAAAI/AAAAAAAAAB4/zjKS1KNld7M/s64/photo.jpg","userId":"00600840190530285990"}},"colab":{"base_uri":"https://localhost:8080/","height":6392}},"cell_type":"code","source":["# viz = visdom.Visdom()\n","\n","net = LeNet5()\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(net.parameters(), lr=2e-3)\n","\n","\n","\n","cur_batch_win = None\n","cur_batch_win_opts = {\n","    'title': 'Epoch Loss Trace',\n","    'xlabel': 'Batch Number',\n","    'ylabel': 'Loss',\n","    'width': 1200,\n","    'height': 600,\n","}\n","\n","\n","def train(epoch):\n","    global cur_batch_win\n","    \n","    # train mode\n","    net.train()\n","    \n","    loss_list, batch_list = [], []\n","    for i, (images, labels) in enumerate(data_train_loader):\n","      \n","#       clear the grad in each loop \n","        optimizer.zero_grad()\n","\n","        output = net(images) # input and get the output \n","\n","        loss = criterion(output, labels)  # compute the loss \n","\n","        loss_list.append(loss.detach().cpu().item())\n","        batch_list.append(i+1)\n","\n","        if i % 10 == 0:\n","            print('Train - Epoch %d, Batch: %d, Loss: %f' % (epoch, i, loss.detach().cpu().item()))\n","\n","        # Update Visualization\n","#         if viz.check_connection():\n","#             cur_batch_win = viz.line(torch.Tensor(loss_list), torch.Tensor(batch_list),\n","#                                      win=cur_batch_win, name='current_batch_loss',\n","#                                      update=(None if cur_batch_win is None else 'replace'),\n","#                                      opts=cur_batch_win_opts)\n","\n","        loss.backward()\n","        optimizer.step()\n","        \n","        \n","def test():\n","#   eval mode\n","    net.eval()\n","  \n","    total_correct = 0\n","    avg_loss = 0.0\n","    for i, (images, labels) in enumerate(data_test_loader):\n","        output = net(images)\n","        avg_loss += criterion(output, labels).sum()\n","        pred = output.detach().max(1)[1]  # get the prediction value\n","        total_correct += pred.eq(labels.view_as(pred)).sum()  # sum all the correct ones\n","\n","    avg_loss /= len(data_test)\n","    print('Test Avg. Loss: %f, Accuracy: %f' % (avg_loss.detach().cpu().item(), float(total_correct) / len(data_test)))\n","\n","    \n","def train_and_test(epoch):\n","    train(epoch)\n","    test()\n","\n","\n","def main():\n","#   16 epoches\n","    for e in range(1, 16):\n","        train_and_test(e)\n","\n","\n","# if __name__ == '__main__':\n","#     main()\n","main()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train - Epoch 1, Batch: 0, Loss: 2.300623\n","Train - Epoch 1, Batch: 10, Loss: 2.007290\n","Train - Epoch 1, Batch: 20, Loss: 1.030814\n","Train - Epoch 1, Batch: 30, Loss: 0.708030\n","Train - Epoch 1, Batch: 40, Loss: 0.491188\n","Train - Epoch 1, Batch: 50, Loss: 0.451606\n","Train - Epoch 1, Batch: 60, Loss: 0.334930\n","Train - Epoch 1, Batch: 70, Loss: 0.289070\n","Train - Epoch 1, Batch: 80, Loss: 0.452226\n","Train - Epoch 1, Batch: 90, Loss: 0.306215\n","Train - Epoch 1, Batch: 100, Loss: 0.259326\n","Train - Epoch 1, Batch: 110, Loss: 0.226109\n","Train - Epoch 1, Batch: 120, Loss: 0.197808\n","Train - Epoch 1, Batch: 130, Loss: 0.289778\n","Train - Epoch 1, Batch: 140, Loss: 0.249778\n","Train - Epoch 1, Batch: 150, Loss: 0.155460\n","Train - Epoch 1, Batch: 160, Loss: 0.238335\n","Train - Epoch 1, Batch: 170, Loss: 0.263578\n","Train - Epoch 1, Batch: 180, Loss: 0.158038\n","Train - Epoch 1, Batch: 190, Loss: 0.088690\n","Train - Epoch 1, Batch: 200, Loss: 0.123026\n","Train - Epoch 1, Batch: 210, Loss: 0.129897\n","Train - Epoch 1, Batch: 220, Loss: 0.169549\n","Train - Epoch 1, Batch: 230, Loss: 0.096112\n","Test Avg. Loss: 0.000135, Accuracy: 0.957600\n","Train - Epoch 2, Batch: 0, Loss: 0.153774\n","Train - Epoch 2, Batch: 10, Loss: 0.121322\n","Train - Epoch 2, Batch: 20, Loss: 0.085334\n","Train - Epoch 2, Batch: 30, Loss: 0.111697\n","Train - Epoch 2, Batch: 40, Loss: 0.045409\n","Train - Epoch 2, Batch: 50, Loss: 0.143898\n","Train - Epoch 2, Batch: 60, Loss: 0.150331\n","Train - Epoch 2, Batch: 70, Loss: 0.112394\n","Train - Epoch 2, Batch: 80, Loss: 0.228027\n","Train - Epoch 2, Batch: 90, Loss: 0.144382\n","Train - Epoch 2, Batch: 100, Loss: 0.071017\n","Train - Epoch 2, Batch: 110, Loss: 0.101278\n","Train - Epoch 2, Batch: 120, Loss: 0.074301\n","Train - Epoch 2, Batch: 130, Loss: 0.104290\n","Train - Epoch 2, Batch: 140, Loss: 0.126305\n","Train - Epoch 2, Batch: 150, Loss: 0.121866\n","Train - Epoch 2, Batch: 160, Loss: 0.087559\n","Train - Epoch 2, Batch: 170, Loss: 0.095189\n","Train - Epoch 2, Batch: 180, Loss: 0.066737\n","Train - Epoch 2, Batch: 190, Loss: 0.106289\n","Train - Epoch 2, Batch: 200, Loss: 0.079127\n","Train - Epoch 2, Batch: 210, Loss: 0.064002\n","Train - Epoch 2, Batch: 220, Loss: 0.087192\n","Train - Epoch 2, Batch: 230, Loss: 0.046627\n","Test Avg. Loss: 0.000074, Accuracy: 0.975000\n","Train - Epoch 3, Batch: 0, Loss: 0.047892\n","Train - Epoch 3, Batch: 10, Loss: 0.068423\n","Train - Epoch 3, Batch: 20, Loss: 0.050917\n","Train - Epoch 3, Batch: 30, Loss: 0.059830\n","Train - Epoch 3, Batch: 40, Loss: 0.067297\n","Train - Epoch 3, Batch: 50, Loss: 0.112648\n","Train - Epoch 3, Batch: 60, Loss: 0.101087\n","Train - Epoch 3, Batch: 70, Loss: 0.077865\n","Train - Epoch 3, Batch: 80, Loss: 0.032808\n","Train - Epoch 3, Batch: 90, Loss: 0.037442\n","Train - Epoch 3, Batch: 100, Loss: 0.064784\n","Train - Epoch 3, Batch: 110, Loss: 0.064418\n","Train - Epoch 3, Batch: 120, Loss: 0.094370\n","Train - Epoch 3, Batch: 130, Loss: 0.048673\n","Train - Epoch 3, Batch: 140, Loss: 0.054470\n","Train - Epoch 3, Batch: 150, Loss: 0.073310\n","Train - Epoch 3, Batch: 160, Loss: 0.057884\n","Train - Epoch 3, Batch: 170, Loss: 0.081210\n","Train - Epoch 3, Batch: 180, Loss: 0.061417\n","Train - Epoch 3, Batch: 190, Loss: 0.077142\n","Train - Epoch 3, Batch: 200, Loss: 0.030730\n","Train - Epoch 3, Batch: 210, Loss: 0.051141\n","Train - Epoch 3, Batch: 220, Loss: 0.063798\n","Train - Epoch 3, Batch: 230, Loss: 0.012518\n","Test Avg. Loss: 0.000059, Accuracy: 0.980800\n","Train - Epoch 4, Batch: 0, Loss: 0.034686\n","Train - Epoch 4, Batch: 10, Loss: 0.077021\n","Train - Epoch 4, Batch: 20, Loss: 0.059616\n","Train - Epoch 4, Batch: 30, Loss: 0.069663\n","Train - Epoch 4, Batch: 40, Loss: 0.086503\n","Train - Epoch 4, Batch: 50, Loss: 0.047709\n","Train - Epoch 4, Batch: 60, Loss: 0.074864\n","Train - Epoch 4, Batch: 70, Loss: 0.015835\n","Train - Epoch 4, Batch: 80, Loss: 0.117119\n","Train - Epoch 4, Batch: 90, Loss: 0.051396\n","Train - Epoch 4, Batch: 100, Loss: 0.086964\n","Train - Epoch 4, Batch: 110, Loss: 0.018932\n","Train - Epoch 4, Batch: 120, Loss: 0.047418\n","Train - Epoch 4, Batch: 130, Loss: 0.017265\n","Train - Epoch 4, Batch: 140, Loss: 0.057793\n","Train - Epoch 4, Batch: 150, Loss: 0.020133\n","Train - Epoch 4, Batch: 160, Loss: 0.039133\n","Train - Epoch 4, Batch: 170, Loss: 0.068947\n","Train - Epoch 4, Batch: 180, Loss: 0.032749\n","Train - Epoch 4, Batch: 190, Loss: 0.058069\n","Train - Epoch 4, Batch: 200, Loss: 0.050201\n","Train - Epoch 4, Batch: 210, Loss: 0.011490\n","Train - Epoch 4, Batch: 220, Loss: 0.034116\n","Train - Epoch 4, Batch: 230, Loss: 0.078793\n","Test Avg. Loss: 0.000041, Accuracy: 0.986700\n","Train - Epoch 5, Batch: 0, Loss: 0.059322\n","Train - Epoch 5, Batch: 10, Loss: 0.069968\n","Train - Epoch 5, Batch: 20, Loss: 0.052709\n","Train - Epoch 5, Batch: 30, Loss: 0.049875\n","Train - Epoch 5, Batch: 40, Loss: 0.051264\n","Train - Epoch 5, Batch: 50, Loss: 0.069640\n","Train - Epoch 5, Batch: 60, Loss: 0.037079\n","Train - Epoch 5, Batch: 70, Loss: 0.016585\n","Train - Epoch 5, Batch: 80, Loss: 0.018958\n","Train - Epoch 5, Batch: 90, Loss: 0.043131\n","Train - Epoch 5, Batch: 100, Loss: 0.039614\n","Train - Epoch 5, Batch: 110, Loss: 0.046860\n","Train - Epoch 5, Batch: 120, Loss: 0.022305\n","Train - Epoch 5, Batch: 130, Loss: 0.073557\n","Train - Epoch 5, Batch: 140, Loss: 0.033725\n","Train - Epoch 5, Batch: 150, Loss: 0.028201\n","Train - Epoch 5, Batch: 160, Loss: 0.020294\n","Train - Epoch 5, Batch: 170, Loss: 0.081418\n","Train - Epoch 5, Batch: 180, Loss: 0.034561\n","Train - Epoch 5, Batch: 190, Loss: 0.034433\n","Train - Epoch 5, Batch: 200, Loss: 0.039331\n","Train - Epoch 5, Batch: 210, Loss: 0.040451\n","Train - Epoch 5, Batch: 220, Loss: 0.010040\n","Train - Epoch 5, Batch: 230, Loss: 0.029643\n","Test Avg. Loss: 0.000031, Accuracy: 0.989100\n","Train - Epoch 6, Batch: 0, Loss: 0.017642\n","Train - Epoch 6, Batch: 10, Loss: 0.035930\n","Train - Epoch 6, Batch: 20, Loss: 0.029248\n","Train - Epoch 6, Batch: 30, Loss: 0.034388\n","Train - Epoch 6, Batch: 40, Loss: 0.063829\n","Train - Epoch 6, Batch: 50, Loss: 0.033982\n","Train - Epoch 6, Batch: 60, Loss: 0.022237\n","Train - Epoch 6, Batch: 70, Loss: 0.060045\n","Train - Epoch 6, Batch: 80, Loss: 0.034045\n","Train - Epoch 6, Batch: 90, Loss: 0.018946\n","Train - Epoch 6, Batch: 100, Loss: 0.015106\n","Train - Epoch 6, Batch: 110, Loss: 0.025496\n","Train - Epoch 6, Batch: 120, Loss: 0.038822\n","Train - Epoch 6, Batch: 130, Loss: 0.020627\n","Train - Epoch 6, Batch: 140, Loss: 0.070703\n","Train - Epoch 6, Batch: 150, Loss: 0.034857\n","Train - Epoch 6, Batch: 160, Loss: 0.040983\n","Train - Epoch 6, Batch: 170, Loss: 0.047123\n","Train - Epoch 6, Batch: 180, Loss: 0.026589\n","Train - Epoch 6, Batch: 190, Loss: 0.054260\n","Train - Epoch 6, Batch: 200, Loss: 0.037683\n","Train - Epoch 6, Batch: 210, Loss: 0.024950\n","Train - Epoch 6, Batch: 220, Loss: 0.071239\n","Train - Epoch 6, Batch: 230, Loss: 0.030959\n","Test Avg. Loss: 0.000033, Accuracy: 0.989900\n","Train - Epoch 7, Batch: 0, Loss: 0.014444\n","Train - Epoch 7, Batch: 10, Loss: 0.020947\n","Train - Epoch 7, Batch: 20, Loss: 0.024564\n","Train - Epoch 7, Batch: 30, Loss: 0.025342\n","Train - Epoch 7, Batch: 40, Loss: 0.031942\n","Train - Epoch 7, Batch: 50, Loss: 0.018972\n","Train - Epoch 7, Batch: 60, Loss: 0.032637\n","Train - Epoch 7, Batch: 70, Loss: 0.022334\n","Train - Epoch 7, Batch: 80, Loss: 0.009201\n","Train - Epoch 7, Batch: 90, Loss: 0.027572\n","Train - Epoch 7, Batch: 100, Loss: 0.022328\n","Train - Epoch 7, Batch: 110, Loss: 0.069289\n","Train - Epoch 7, Batch: 120, Loss: 0.030923\n","Train - Epoch 7, Batch: 130, Loss: 0.028360\n","Train - Epoch 7, Batch: 140, Loss: 0.060690\n","Train - Epoch 7, Batch: 150, Loss: 0.034040\n","Train - Epoch 7, Batch: 160, Loss: 0.016715\n","Train - Epoch 7, Batch: 170, Loss: 0.044524\n","Train - Epoch 7, Batch: 180, Loss: 0.037216\n","Train - Epoch 7, Batch: 190, Loss: 0.029176\n","Train - Epoch 7, Batch: 200, Loss: 0.020392\n","Train - Epoch 7, Batch: 210, Loss: 0.026337\n","Train - Epoch 7, Batch: 220, Loss: 0.036948\n","Train - Epoch 7, Batch: 230, Loss: 0.042269\n","Test Avg. Loss: 0.000028, Accuracy: 0.990700\n","Train - Epoch 8, Batch: 0, Loss: 0.012244\n","Train - Epoch 8, Batch: 10, Loss: 0.005891\n","Train - Epoch 8, Batch: 20, Loss: 0.027766\n","Train - Epoch 8, Batch: 30, Loss: 0.005856\n","Train - Epoch 8, Batch: 40, Loss: 0.018112\n","Train - Epoch 8, Batch: 50, Loss: 0.011351\n","Train - Epoch 8, Batch: 60, Loss: 0.052922\n","Train - Epoch 8, Batch: 70, Loss: 0.011267\n","Train - Epoch 8, Batch: 80, Loss: 0.023033\n","Train - Epoch 8, Batch: 90, Loss: 0.029800\n","Train - Epoch 8, Batch: 100, Loss: 0.050232\n","Train - Epoch 8, Batch: 110, Loss: 0.034357\n","Train - Epoch 8, Batch: 120, Loss: 0.056003\n","Train - Epoch 8, Batch: 130, Loss: 0.042805\n","Train - Epoch 8, Batch: 140, Loss: 0.011453\n","Train - Epoch 8, Batch: 150, Loss: 0.096607\n","Train - Epoch 8, Batch: 160, Loss: 0.014118\n","Train - Epoch 8, Batch: 170, Loss: 0.056727\n","Train - Epoch 8, Batch: 180, Loss: 0.029616\n","Train - Epoch 8, Batch: 190, Loss: 0.017799\n","Train - Epoch 8, Batch: 200, Loss: 0.043856\n","Train - Epoch 8, Batch: 210, Loss: 0.037877\n","Train - Epoch 8, Batch: 220, Loss: 0.028551\n","Train - Epoch 8, Batch: 230, Loss: 0.034629\n","Test Avg. Loss: 0.000031, Accuracy: 0.989100\n","Train - Epoch 9, Batch: 0, Loss: 0.010071\n","Train - Epoch 9, Batch: 10, Loss: 0.038594\n","Train - Epoch 9, Batch: 20, Loss: 0.014372\n","Train - Epoch 9, Batch: 30, Loss: 0.012568\n","Train - Epoch 9, Batch: 40, Loss: 0.016911\n","Train - Epoch 9, Batch: 50, Loss: 0.013076\n","Train - Epoch 9, Batch: 60, Loss: 0.017718\n","Train - Epoch 9, Batch: 70, Loss: 0.022091\n","Train - Epoch 9, Batch: 80, Loss: 0.032858\n","Train - Epoch 9, Batch: 90, Loss: 0.004205\n","Train - Epoch 9, Batch: 100, Loss: 0.048115\n","Train - Epoch 9, Batch: 110, Loss: 0.037688\n","Train - Epoch 9, Batch: 120, Loss: 0.037336\n","Train - Epoch 9, Batch: 130, Loss: 0.031007\n","Train - Epoch 9, Batch: 140, Loss: 0.021202\n","Train - Epoch 9, Batch: 150, Loss: 0.022370\n","Train - Epoch 9, Batch: 160, Loss: 0.018695\n","Train - Epoch 9, Batch: 170, Loss: 0.020277\n","Train - Epoch 9, Batch: 180, Loss: 0.036930\n","Train - Epoch 9, Batch: 190, Loss: 0.006712\n","Train - Epoch 9, Batch: 200, Loss: 0.021448\n","Train - Epoch 9, Batch: 210, Loss: 0.039374\n","Train - Epoch 9, Batch: 220, Loss: 0.033251\n","Train - Epoch 9, Batch: 230, Loss: 0.014479\n","Test Avg. Loss: 0.000039, Accuracy: 0.987400\n","Train - Epoch 10, Batch: 0, Loss: 0.024461\n","Train - Epoch 10, Batch: 10, Loss: 0.034696\n","Train - Epoch 10, Batch: 20, Loss: 0.042119\n","Train - Epoch 10, Batch: 30, Loss: 0.037472\n","Train - Epoch 10, Batch: 40, Loss: 0.036573\n","Train - Epoch 10, Batch: 50, Loss: 0.011252\n","Train - Epoch 10, Batch: 60, Loss: 0.011340\n","Train - Epoch 10, Batch: 70, Loss: 0.012392\n","Train - Epoch 10, Batch: 80, Loss: 0.010006\n","Train - Epoch 10, Batch: 90, Loss: 0.001274\n","Train - Epoch 10, Batch: 100, Loss: 0.026045\n","Train - Epoch 10, Batch: 110, Loss: 0.026404\n","Train - Epoch 10, Batch: 120, Loss: 0.015206\n","Train - Epoch 10, Batch: 130, Loss: 0.029090\n","Train - Epoch 10, Batch: 140, Loss: 0.029596\n","Train - Epoch 10, Batch: 150, Loss: 0.036047\n","Train - Epoch 10, Batch: 160, Loss: 0.019520\n","Train - Epoch 10, Batch: 170, Loss: 0.023912\n","Train - Epoch 10, Batch: 180, Loss: 0.025600\n","Train - Epoch 10, Batch: 190, Loss: 0.040061\n","Train - Epoch 10, Batch: 200, Loss: 0.035182\n","Train - Epoch 10, Batch: 210, Loss: 0.005452\n","Train - Epoch 10, Batch: 220, Loss: 0.022091\n","Train - Epoch 10, Batch: 230, Loss: 0.006802\n","Test Avg. Loss: 0.000024, Accuracy: 0.992200\n","Train - Epoch 11, Batch: 0, Loss: 0.011610\n","Train - Epoch 11, Batch: 10, Loss: 0.016974\n","Train - Epoch 11, Batch: 20, Loss: 0.003670\n","Train - Epoch 11, Batch: 30, Loss: 0.005202\n","Train - Epoch 11, Batch: 40, Loss: 0.037528\n","Train - Epoch 11, Batch: 50, Loss: 0.009441\n","Train - Epoch 11, Batch: 60, Loss: 0.012758\n","Train - Epoch 11, Batch: 70, Loss: 0.028072\n","Train - Epoch 11, Batch: 80, Loss: 0.015068\n","Train - Epoch 11, Batch: 90, Loss: 0.018918\n","Train - Epoch 11, Batch: 100, Loss: 0.022677\n","Train - Epoch 11, Batch: 110, Loss: 0.003463\n","Train - Epoch 11, Batch: 120, Loss: 0.033913\n","Train - Epoch 11, Batch: 130, Loss: 0.045714\n","Train - Epoch 11, Batch: 140, Loss: 0.007471\n","Train - Epoch 11, Batch: 150, Loss: 0.020419\n","Train - Epoch 11, Batch: 160, Loss: 0.017969\n","Train - Epoch 11, Batch: 170, Loss: 0.011376\n","Train - Epoch 11, Batch: 180, Loss: 0.010624\n","Train - Epoch 11, Batch: 190, Loss: 0.011105\n","Train - Epoch 11, Batch: 200, Loss: 0.002948\n","Train - Epoch 11, Batch: 210, Loss: 0.027447\n","Train - Epoch 11, Batch: 220, Loss: 0.028426\n","Train - Epoch 11, Batch: 230, Loss: 0.014035\n","Test Avg. Loss: 0.000028, Accuracy: 0.989900\n","Train - Epoch 12, Batch: 0, Loss: 0.027794\n","Train - Epoch 12, Batch: 10, Loss: 0.019322\n","Train - Epoch 12, Batch: 20, Loss: 0.012710\n","Train - Epoch 12, Batch: 30, Loss: 0.007688\n","Train - Epoch 12, Batch: 40, Loss: 0.028928\n","Train - Epoch 12, Batch: 50, Loss: 0.013003\n","Train - Epoch 12, Batch: 60, Loss: 0.006751\n","Train - Epoch 12, Batch: 70, Loss: 0.029420\n","Train - Epoch 12, Batch: 80, Loss: 0.019032\n","Train - Epoch 12, Batch: 90, Loss: 0.003905\n","Train - Epoch 12, Batch: 100, Loss: 0.008605\n","Train - Epoch 12, Batch: 110, Loss: 0.009298\n","Train - Epoch 12, Batch: 120, Loss: 0.025224\n","Train - Epoch 12, Batch: 130, Loss: 0.019705\n","Train - Epoch 12, Batch: 140, Loss: 0.014238\n","Train - Epoch 12, Batch: 150, Loss: 0.010420\n","Train - Epoch 12, Batch: 160, Loss: 0.032299\n","Train - Epoch 12, Batch: 170, Loss: 0.027215\n","Train - Epoch 12, Batch: 180, Loss: 0.054249\n","Train - Epoch 12, Batch: 190, Loss: 0.001684\n","Train - Epoch 12, Batch: 200, Loss: 0.027447\n","Train - Epoch 12, Batch: 210, Loss: 0.009744\n","Train - Epoch 12, Batch: 220, Loss: 0.040879\n","Train - Epoch 12, Batch: 230, Loss: 0.014503\n","Test Avg. Loss: 0.000032, Accuracy: 0.989900\n","Train - Epoch 13, Batch: 0, Loss: 0.006047\n","Train - Epoch 13, Batch: 10, Loss: 0.021611\n","Train - Epoch 13, Batch: 20, Loss: 0.005655\n","Train - Epoch 13, Batch: 30, Loss: 0.011175\n","Train - Epoch 13, Batch: 40, Loss: 0.003144\n","Train - Epoch 13, Batch: 50, Loss: 0.006565\n","Train - Epoch 13, Batch: 60, Loss: 0.007172\n","Train - Epoch 13, Batch: 70, Loss: 0.052048\n","Train - Epoch 13, Batch: 80, Loss: 0.046574\n","Train - Epoch 13, Batch: 90, Loss: 0.002109\n","Train - Epoch 13, Batch: 100, Loss: 0.022386\n","Train - Epoch 13, Batch: 110, Loss: 0.015459\n","Train - Epoch 13, Batch: 120, Loss: 0.022050\n","Train - Epoch 13, Batch: 130, Loss: 0.004454\n","Train - Epoch 13, Batch: 140, Loss: 0.019525\n","Train - Epoch 13, Batch: 150, Loss: 0.003239\n","Train - Epoch 13, Batch: 160, Loss: 0.010875\n","Train - Epoch 13, Batch: 170, Loss: 0.015989\n","Train - Epoch 13, Batch: 180, Loss: 0.032619\n","Train - Epoch 13, Batch: 190, Loss: 0.049201\n","Train - Epoch 13, Batch: 200, Loss: 0.011974\n","Train - Epoch 13, Batch: 210, Loss: 0.008497\n","Train - Epoch 13, Batch: 220, Loss: 0.013238\n","Train - Epoch 13, Batch: 230, Loss: 0.025225\n","Test Avg. Loss: 0.000027, Accuracy: 0.990900\n","Train - Epoch 14, Batch: 0, Loss: 0.004576\n","Train - Epoch 14, Batch: 10, Loss: 0.005196\n","Train - Epoch 14, Batch: 20, Loss: 0.010263\n","Train - Epoch 14, Batch: 30, Loss: 0.006313\n","Train - Epoch 14, Batch: 40, Loss: 0.003533\n","Train - Epoch 14, Batch: 50, Loss: 0.008000\n","Train - Epoch 14, Batch: 60, Loss: 0.009660\n","Train - Epoch 14, Batch: 70, Loss: 0.020199\n","Train - Epoch 14, Batch: 80, Loss: 0.002553\n","Train - Epoch 14, Batch: 90, Loss: 0.020821\n","Train - Epoch 14, Batch: 100, Loss: 0.041778\n","Train - Epoch 14, Batch: 110, Loss: 0.013784\n","Train - Epoch 14, Batch: 120, Loss: 0.051032\n","Train - Epoch 14, Batch: 130, Loss: 0.016258\n","Train - Epoch 14, Batch: 140, Loss: 0.018217\n","Train - Epoch 14, Batch: 150, Loss: 0.004310\n","Train - Epoch 14, Batch: 160, Loss: 0.009132\n","Train - Epoch 14, Batch: 170, Loss: 0.021138\n","Train - Epoch 14, Batch: 180, Loss: 0.011107\n","Train - Epoch 14, Batch: 190, Loss: 0.017023\n","Train - Epoch 14, Batch: 200, Loss: 0.036973\n","Train - Epoch 14, Batch: 210, Loss: 0.013784\n","Train - Epoch 14, Batch: 220, Loss: 0.000916\n","Train - Epoch 14, Batch: 230, Loss: 0.034718\n","Test Avg. Loss: 0.000042, Accuracy: 0.988200\n","Train - Epoch 15, Batch: 0, Loss: 0.006133\n","Train - Epoch 15, Batch: 10, Loss: 0.004345\n","Train - Epoch 15, Batch: 20, Loss: 0.049745\n","Train - Epoch 15, Batch: 30, Loss: 0.027215\n","Train - Epoch 15, Batch: 40, Loss: 0.018145\n","Train - Epoch 15, Batch: 50, Loss: 0.018332\n","Train - Epoch 15, Batch: 60, Loss: 0.000357\n","Train - Epoch 15, Batch: 70, Loss: 0.002167\n","Train - Epoch 15, Batch: 80, Loss: 0.010989\n","Train - Epoch 15, Batch: 90, Loss: 0.005991\n","Train - Epoch 15, Batch: 100, Loss: 0.028273\n","Train - Epoch 15, Batch: 110, Loss: 0.018830\n","Train - Epoch 15, Batch: 120, Loss: 0.001091\n","Train - Epoch 15, Batch: 130, Loss: 0.021988\n","Train - Epoch 15, Batch: 140, Loss: 0.008304\n","Train - Epoch 15, Batch: 150, Loss: 0.018739\n","Train - Epoch 15, Batch: 160, Loss: 0.017320\n","Train - Epoch 15, Batch: 170, Loss: 0.047536\n","Train - Epoch 15, Batch: 180, Loss: 0.004092\n","Train - Epoch 15, Batch: 190, Loss: 0.008039\n","Train - Epoch 15, Batch: 200, Loss: 0.006520\n","Train - Epoch 15, Batch: 210, Loss: 0.048755\n","Train - Epoch 15, Batch: 220, Loss: 0.013891\n","Train - Epoch 15, Batch: 230, Loss: 0.024434\n","Test Avg. Loss: 0.000030, Accuracy: 0.990700\n"],"name":"stdout"}]},{"metadata":{"id":"GSf_zFUo1zd7","colab_type":"text"},"cell_type":"markdown","source":[""]}]}